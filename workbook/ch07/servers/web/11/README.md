
## The Future of the Web

If we allow ourselves to speculate about the future of the web, we might anticipate changes both in the
near term and further ahead. You may have other thoughts, so feel free to reflect on and comment upon
the following ideas .. or add your own!

The web might bifurcate into:
- LLM-optimised, semantically rich, dynamically interpreted knowledge substrates, and
- Authenticity-preserving, human-authored, cryptographically signed cultural archives.

Between them, dynamic agents--possibly embodied in browsers, shells, or assistants--will mediate interaction.
The role of HTML may diminish in favor of formats designed for LLM consumption or direct interaction.
Let's explore this in more detail.


#### 1. Web Adapted to LLMs

The web might evolve to accommodate language models--just as it adapted to mobile and search engines.

Possible Trends:
- LLM-optimised markup: Simplified, semantically rich formats like Markdown (or variants)
  may be favored over HTML. They're easier for LLMs to parse and regenerate.
- Structured semantification: Expect a massive rise in lightweight metadata (e.g. JSON-LD, RDFa)
  and hybrid vector-symbolic representations to make content better understood and referenced by LLMs.
- "LLM-ready" documents: Authors may provide parallel versions of content--one for rendering,
  one for LLM consumption--much like alt text or aria-labels.


#### 2. New Forms of Dynamic Web Pages

Traditional dynamic content (e.g. JavaScript + server-rendered HTML) may give way to prompt-responsive
pages and even directly executable knowledge objects.

Speculative Ideas:
- LLM-native documents: Pages that contain embedded prompts, vector hints, or partial completions--designed
  to be completed or tailored on access by a local/remote LLM.
- Hyperprompting: Instead of clicking links, users engage in recursive prompt-chains--e.g. "Read this page,"
  followed by "summarize only what's new since 2023," or "adjust for 12-year-olds."
- AI-executables (pseudo-code + data): A new kind of page format may blend code, natural language, and
  datasets into a directly "interpretable" experience. Think literate programming meets REST meets REPL.

You might also compare current developments, to what I imagined in 1998 on what I called
"[object browsing](./1998/)".


#### 3. The Contamination Divide: Human vs. AI-generated Content

A major divide is already forming and will harden. This is partly about trust, provenance, and curation.

Likely Futures:
- Content Provenance Tracking: With the rise of standards like C2PA (https://c2pa.org), we'll see
  embedded cryptographic signatures tracing authorship and edit history.
- Toxicity Filters, but for Origin: Systems will offer filters: "Show only human-written content,"
  "Exclude AI rehashes," or conversely, "Summarise 1000 AI docs in a flash."
- Academic/archival preservation of "pure" corpora: New libraries and institutions will maintain
  "human-origin" corpora for scientific, linguistic, or cultural research.


#### 4. Post-Browser Interfaces: Beyond HTML

As LLMs integrate more deeply into interfaces, the browser itself may become vestigial.

Alternatives:
- Conversational OS shells: Interfaces where users describe intentions ("show me trends in Scandinavian
  languages in news articles") and the system assembles a display, drawing from APIs, text corpora,
  and visualizations.
- Agent-based browsers: Instead of browsing manually, you deploy an LLM-powered agent that curates,
  interprets, and summarizes content before you even "see" it.
- Vector-native navigation: Search and hyperlinks could evolve into multidimensional embedding
  maps--users browse semantically instead of by URL.


#### 5. Unsettling Possibilities and Weird Futures

Let's indulge in some hallucinations:
- Executable Markdown or Hybrid Markdown-DSLs: Markdown fused with reactive programming primitives
  (e.g., [[reactive-code: ..]]) could create interactive documents that render, execute, and explain themselves.
- AI-Native Web Archives: "LLM Archives" where only tokenized or vectorized form of knowledge is
  preserved--compressing millions of texts into dense neural indices.
- Counterfeit Webs: Entire fake internets tailored to you--a personalised hallucinated web (think:
  simulacra of Medium, Stack Overflow, Wikipedia) trained to match your biases, comfort, and curiosity.
  A blessing, or possible disaster?
- Strictly Human-Only Zones: Just as the "slow food" movement emphasises unprocessed, deliberate consumption,
  we may see "slow knowledge" spaces emerge--areas of the web (or outside it) that prohibit AI-generated content,
  automated summaries, or algorithmic personalisation, preserving fully human-authored and human-curated knowledge.


#### 6. Example of Reasonable Development: The Semantic Web

Tim Berners-Lee originally envisioned a World Wide Web that extended beyond syntactic structure to
incorporate semantic understanding. While the initial development of this "semantic web" was curtailed
and never fully realized through its intended technical frameworks, certain conceptual elements endured.
With the advent of large language models (LLMs), the [semantic web](./SEMANTICS.md) appears to be
approaching realisation--albeit through means quite different from those Berners-Lee initially proposed.




## The Debate Over Deskilling and AI in Programming: A Balanced Perspective

The rise of AI coding assistants and low-code platforms has sparked an intense
debate about the future of software development. On one side, proponents argue
we're witnessing a positive transformation that will elevate the profession.
On the other, critics warn of a dangerous erosion of fundamental skills.
The truth probably lies somewhere in between, with both perspectives offering
valid insights worth examining.


### The Case for Optimism

Those who support AI-assisted development argue from economic scarcity. Demand
for software far exceeds the supply of qualified developers, and low-code and
AI tools are less about replacing engineers than about removing this
bottleneck. By automating routine work and enabling "citizen developers",
they expand the effective workforce and allow professionals to focus on more
valuable tasks.

Market evidence supports this view. Developers who use low-code platforms
often earn more and report higher job satisfaction. Rather than devaluing
their skills, these tools shift their work away from repetitive
implementation towards architecture, strategy, and creative problem-solving.

History suggests this pattern is normal. During the Industrial Revolution,
automation did not eliminate skilled labour but moved complexity upward into
new roles. Likewise, as programming becomes more automated, routine coding is
handled by tools while human expertise concentrates on design, integration,
and oversight. The developer becomes more of a system orchestrator than a
manual coder.

Efficiency gains reinforce the case. Faster development means quicker
delivery, better responsiveness to change, and more resources for innovation.
Many developers report significantly reduced backlogs, giving them space for
challenging and intellectually meaningful work.

At the margin, AI may still fail to meet all software requirements, so these
arguments apply mainly to large-scale, mainstream software development rather
than every possible domain.


### The Case for Concern

Critics of AI-assisted development argue that it risks eroding foundational
knowledge. When developers rely heavily on high-level abstractions and
AI-generated code, they may never gain a deep understanding of algorithms,
data structures, or system architecture. This creates a "skill trap" in which
practitioners become dependent on tools whose inner workings they do not
understand.

Debugging makes this especially clear. With self-written code, developers can
reason about intent and failure. AI-generated code, while often syntactically
correct, may contain subtle semantic or performance errors. Debugging then
becomes the problem of understanding a black box. Without grounding in memory
management or algorithmic complexity, developers struggle to diagnose
bottlenecks or inefficiencies at scale.

Security adds a further risk. AI tools may reproduce vulnerabilities from their
training data or ignore current best practices and regulatory requirements.
Reports already show widespread concern and real incidents related to
AI-generated code, indicating this is not a theoretical problem.

The broader danger is systemic. If most developers lose the ability to reason
about underlying systems, the profession becomes fragile and dependent on
opaque, often proprietary tools. A small elite may retain deep expertise while
the majority become tool operators, unable to act when abstractions fail.

Education is particularly vulnerable. Difficult concepts like pointers, data
structures, and algorithmic reasoning have traditionally built strong
problem-solving skills. By hiding this complexity, AI tools make programming
easier to enter but risk preventing students from ever developing genuine
mastery, replacing understanding with dependency.


### Real-World Examples Illustrating Both Perspectives

Take database queries. An AI can quickly generate a working SQL query, saving time
and letting developers focus on higher-level design. From an optimistic view, this
is a clear efficiency gain. But the same query may perform poorly at scale. Without
knowledge of indexing, optimisation, and execution plans, a developer cannot judge
whether the solution is sound. When performance problems appear later, they may be
unable to diagnose or fix them without external help.

Security provides a similar contrast. AI can generate authentication code that seems
to work, which looks like a major step towards making secure systems easier to build.
Yet if the code reproduces insecure patterns, such as weak password handling or poor
input validation, a developer without security fundamentals may deploy serious
vulnerabilities without realising it.

Low-code platforms show the same duality. A business user can build an application
quickly, reducing cost and relieving the IT backlog. At the same time, the result may
ignore data governance, error handling, or long-term maintainability. What begins as
an efficiency gain can later become technical debt and integration problems.

In each case, the tools increase speed and accessibility, but they also amplify the
cost of missing foundational knowledge.


### Synthesis and Path Forward

The debate is not a simple choice between two opposing positions. Both capture
important aspects of a complex transformation. AI and automation are at once
empowering and potentially deskilling. They can elevate the profession while also
introducing new forms of dependency and vulnerability. The decisive factor is not
the technology itself, but how it is adopted, taught, and governed.

The idea of *hybrid intelligence* provides a useful framework. This is not a
competition between humans and machines, nor a story of replacement, but one of
collaboration. AI handles routine and repetitive implementation, while humans
contribute strategic judgement, architectural vision, and the deep understanding
required when abstractions fail. The challenge is to preserve and cultivate this
deep understanding even as tools make it less visible in everyday work.

Achieving this demands deliberate effort. Education must balance tool proficiency
with foundational knowledge, creating a kind of "pedagogy of contrast" in which
students both use powerful abstractions and learn what those abstractions conceal.
They should understand not only how to work with modern frameworks, but also what
would be required without them.

Organisations face a similar responsibility. Continuous professional development
must go beyond learning new tools to sustaining and deepening core computational
competence. Job roles and career paths should reward architectural insight,
conceptual clarity, and system-level reasoning, not merely productivity measured in
output or familiarity with fashionable technologies.

The parallel with the Industrial Revolution is instructive, but incomplete. Then,
as now, automation shifted complexity rather than eliminating it. Yet the transition
was neither automatic nor painless. It required sustained investment in education,
new professional standards, and institutional support. The same level of
intentionality is required today.

The real question, therefore, is not whether AI will deskill programming. It is
whether we will manage this transition wisely: using AI to amplify human capability
while safeguarding the expertise that makes such amplification meaningful in the
first place.


### The Question of Craft in Programming

Programming as a craft has traditionally been associated with mastery, pride, and
the satisfaction of building systems through deep understanding rather than mere
tool use. The craft programmer knows their materials: data structures, algorithms,
and system behaviour. They value elegance, clarity, and correctness, and take pride
not only in producing working software but in how that software is constructed.

AI assistance appears to challenge this ideal. When tools can instantly generate
algorithms or queries, there is a fear that skills will erode and that the formative
experience of struggling toward understanding will be lost. For many developers,
moments of insight gained through difficulty were not only technical achievements
but foundations of professional identity. Removing that friction risks weakening
both mastery and authorship.

At the same time, craft has never meant writing everything by hand. A carpenter is
not less skilled for using prefabricated materials, and a developer is not less of a
craftsperson for using libraries or higher-level languages. Craft in programming has
always involved choosing the right abstractions and composing complex systems wisely.
AI tools may simply push this further, shifting craftsmanship from line-by-line
construction to system-level design and judgement.

In this sense, craft is not diminished but transformed. It now includes the ability
to specify intent clearly, to evaluate and refine AI-generated code, to integrate
components coherently, and to distinguish between code that merely works and code
that is truly sound. The emphasis moves from production to responsibility for the
whole system.

There is also a pragmatic dimension. Software engineering balances speed, quality,
security, maintainability, and cost. Insisting on handcrafted solutions when
automation is sufficient can be self-indulgent. True craftsmanship lies in knowing
when automation is appropriate and when deeper manual engagement is required.

Yet craft also fulfils human and cultural needs. People find meaning in seeing a clear
connection between their skills and the results they produce. If developers become
only supervisors of machine output, that sense of authorship may weaken, along with
shared professional standards of excellence.

Modern craftsmanship therefore increasingly involves not only writing code, but
verifying it. Techniques such as dependent types, category-theoretic foundations,
property-based testing, model checking, and formal verification provide new arenas
for mastery. They shift the focus from "Did I write this cleverly?" to "Can I show
that this behaves correctly under all relevant conditions?"

For the craft-oriented programmer, these methods are not a retreat from creativity
but an expansion of it. The craft moves from shaping code alone to shaping guarantees:
making correctness explicit and building systems whose behaviour is not merely
plausible, but demonstrably reliable.


### Synthesis and Path Forward

The debate ultimately isn't binary. Both perspectives capture genuine aspects of a
complex transformation. AI and automation tools are simultaneously empowering and
potentially deskilling, elevating the profession while creating new vulnerabilities.
The outcome depends less on the technology itself and more on how individuals
and organisations choose to adopt it.

The concept of "hybrid intelligence" offers perhaps the most promising framework.
Rather than viewing this as humans versus machines or even humans replaced by machines,
we might understand it as humans with machines achieving outcomes neither could
accomplish alone. In this model, AI handles routine implementation while humans
provide strategic direction, architectural vision, and the deep knowledge needed
when abstractions prove insufficient. The key is maintaining that deep knowledge
even while leveraging tools that make it less necessary for routine tasks.

This requires intentional effort. Educational institutions must teach both tool
proficiency and foundational principles, creating what might be called a "pedagogy
of contrast" where students learn to use modern frameworks but also understand
what those frameworks abstract away. Organisations must invest in continuous
upskilling, ensuring developers don't just learn new tools but maintain and
deepen their understanding of core computer science principles. Career paths
and job descriptions need reframing to reward strategic thinking and architectural
skill rather than just lines of code produced or years spent with specific
technologies.

The historical parallel to the Industrial Revolution remains instructive but
incomplete. Yes, complexity migrated upward into new skilled roles. But that
transition was neither automatic nor painless. It required deliberate investment
in education, new professional standards, and social structures to support the
transformation. The same intentionality is needed now. The question isn't whether
AI will deskill programming but whether we'll manage the transition wisely, 
reserving and elevating essential human expertise even as we embrace powerful
new tools.


### Discussion Topics for Reflection

Now that you've explored the various dimensions of deskilling, automation, and
craftsmanship in programming, it's worth engaging more deeply with these ideas
through critical reflection. The following topics are designed to help you think
through the tensions and complexities we've examined, connecting them to your
own experiences and aspirations as a developer or student of computer science.

__On Historical Patterns and Future Trajectories__: The Industrial Revolution
displaced skilled artisans but created new professional classes of engineers,
managers, and technicians. Here we argue that programming is undergoing
a similar polarisation rather than simple elimination. But consider whether
this historical parallel holds up under scrutiny. Are there important differences
between physical manufacturing and knowledge work that might make the analogy
break down? When complexity "migrated upward" during industrialisation, it
took generations and involved significant social upheaval. What does this
suggest about the pace and pain of the current transition? You might also
consider whether the concentration of expertise in fewer hands represents
genuine progress or creates new forms of inequality and vulnerability in the
technology sector.

__On the Security Paradox__: The text highlights that AI-generated code can
reproduce known vulnerabilities and fail to implement current security best
practices, with over half of organisations experiencing security issues from
AI code. Yet these same tools are marketed as making secure development more
accessible. How do you reconcile this tension? Is it possible that AI tools
make security both better and worse simultaneously, perhaps securing routine
cases while creating new vulnerability patterns? Think about who bears
responsibility when AI-generated code causes a security breach. Should it be
the developer who accepted the suggestion, the company that deployed it, or
the AI tool provider? This question touches on broader issues of accountability
in increasingly automated systems.

__On the Nature of Understanding__: A central concern in the deskilling debate
is whether developers using high-level abstractions and AI assistance truly
understand what their code does. But what does "understanding" mean in this
context? When you use a sorting library, do you need to understand the specific
algorithm it implements, or is it sufficient to understand its time complexity
and when to use it? Where do you draw the line between acceptable abstraction
and dangerous ignorance? Consider also that expert developers routinely use
tools and libraries whose internals they don't fully understand. Is there a
meaningful difference between relying on a well-established library and relying
on AI-generated code, or is this distinction somehow arbitrary?

__On Craftsmanship and Meaning__: The section above on craft explores whether
programming is losing something essential when AI automates code creation,
or whether craft is simply evolving to a higher level of abstraction.
Reflect on your own experience with programming. Do you find satisfaction
in the act of writing code itself, or primarily in solving problems and
creating functional systems? If you've used AI coding assistants,
did they enhance or diminish your sense of accomplishment?
Think about what makes work meaningful to you. Is there something intrinsically
valuable about the struggle to implement a solution from scratch, or is this a
form of unnecessary difficulty that we should be happy to automate away? How
might your answer change if you were working under deadline pressure versus
learning for personal growth?

__On the Education Dilemma__: The text proposes a "pedagogy of contrast" where
students learn both modern tools and foundational principles. But this approach
assumes students have time and motivation to learn both. In reality, students
face pressures to become job-ready quickly and may question why they should learn
"outdated" skills when AI can handle them. How would you respond to a fellow
student who asks why they should spend weeks mastering data structures and
algorithms when they can just prompt an AI to generate efficient code? Is there
a way to make the case for foundational knowledge that goes beyond appeals to
professional virtue or warnings about hypothetical future scenarios? Consider
also whether educational institutions can realistically teach everything, or
whether we need to accept that some knowledge will be lost as new skills
become necessary.

__On Market Signals and Professional Evolution__: The evidence shows that
developers who adopt low-code platforms earn higher salaries and report
greater job satisfaction. This suggests the market is rewarding tool adoption
and strategic thinking over traditional coding skills. Yet the reasoning also
warns about erosion of foundational expertise. How do you navigate this tension
in planning your own career? Should you focus on learning the tools and
approaches the market currently rewards, even if they might contribute to
long-term deskilling? Or should you invest in deep foundational knowledge
that might be less immediately marketable? Think about whether it's possible
to pursue both paths simultaneously, and what trade-offs you might need to
accept. Consider also whether early-career developers face different incentives
than experienced professionals, and whether the optimal strategy changes over
the course of a career.

__On System-Level Fragility__: The text argues that individual deskilling creates
collective vulnerability, making entire technological infrastructures dependent
on external tools and the small elite who understand foundational principles.
But is this necessarily bad? Modern society depends on countless systems that
few people understand completely, from electrical grids to pharmaceutical
supply chains. We've developed institutional structures, regulations, and
professional standards to manage these dependencies. Could similar structures
work for programming? Or is there something uniquely dangerous about deskilling
in software development? Think about what happens when abstractions fail or
prove insufficient for novel problems. Who will have the knowledge to innovate?
Is it realistic to expect most developers to maintain deep foundational knowledge
they rarely use, or should we accept a more specialised profession where different
tiers have different depths of understanding?

__On the Citizen Developer Phenomenon__: Low-code platforms are enabling "citizen
developers" with minimal coding experience to build applications, and they're
projected to outnumber professional developers four to one. Some see this as
democratisation of technology, others as a quality and security disaster waiting
to happen. Where do you stand? Consider the analogy to other fields. Desktop
publishing tools allowed non-designers to create documents, sometimes with
terrible results but also democratising access to design capabilities. Personal
finance software enables people to manage investments without professional
financial advisors, with mixed outcomes. What makes programming similar to or
different from these cases? Think about whether there are certain types of
applications that citizen developers should absolutely not build, or whether
with proper guardrails and templates, they can safely handle more than skeptics
believe.

__On Debugging as the Core Skill__: Several sections emphasise that debugging
AI-generated code requires different and perhaps more sophisticated skills than
debugging code you wrote yourself. You must understand the context, dependencies,
and potential failure modes of code whose internal logic you didn't design. Is
debugging, rather than creation, becoming the central skill of modern programming?
If so, what does this mean for how programming should be taught? Should curricula
focus more on reading and analysing existing code rather than writing from scratch?
Consider your own experience with debugging. What makes it difficult? Is it
harder to debug code you didn't write, code written in an unfamiliar style,
or code whose purpose you don't fully understand? How might these challenges
multiply when the code comes from an AI that doesn't explain its reasoning?

__On the Definition of Senior Developer__: The document suggests we need to redefine
what "senior" means in programming, shifting from years of experience with specific
technologies to strategic architectural thinking. But how do you evaluate and
credential these higher-order skills? It's relatively easy to test whether someone
can implement a binary search tree or knows the syntax of a particular language.
It's much harder to assess whether they have good architectural judgment or can
effectively prompt and validate AI systems. How should organisations and the
profession more broadly handle this transition? Should certifications and interviews
change? What about mentorship relationships, when the mentor's deep coding knowledge
may be less relevant but their strategic thinking is hard to transfer? Think about
what you would want from a senior developer on your team. Would you prefer someone
with deep technical knowledge who can dive into complex debugging, or someone with
broad architectural vision who might rely more on tools for implementation?

__On Personal Agency and Skill Investment__: Ultimately, each developer must make
individual choices about which skills to develop and which tools to adopt. The
document presents both opportunities and risks, but you must navigate these fo
yourself. How do you decide whether to lean heavily into AI assistance or maintain
a more traditional skill set? What role does personal interest versus market
pragmatism play in your decision? Consider that your choices don't just affect
your own career but collectively shape the profession's future. If everyone chooses
short-term efficiency over foundational knowledge, the collective outcome might b
professional deskilling. If everyone insists on mastering low-level details that
AI can handle, the profession might become inefficient and fail to meet market
needs. How do you balance personal optimisation with collective professional
health? Is this even your responsibility, or should you focus purely on your
own career success and let broader trends take care of themselves?

These topics don't have simple right answers. They're meant to help you engage critically
with the transformations happening in programming and think through their implications for
your own learning and career. The tensions they explore, between efficiency and expertise,
between democratisation and quality, between adaptation and preservation, will likely
define the programming profession for years to come. Your thoughtful engagement with
these questions will help you navigate this evolving landscape more intentionally and
perhaps contribute to shaping it in positive directions.



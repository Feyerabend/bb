
## Word Embeddings

Self-Supervised Learning

Word embeddings, particularly those generated by models like Word2Vec, are a fundamental concept
in natural language processing (NLP) that allow computers to understand the meaning and relationships
between words.

Imagine you want to represent words not as discrete symbols, but as points in a multi-dimensional
space. That's essentially what word embeddings do.


*The Core Idea:*

Word2Vec (and similar models) learns to represent words as dense, low-dimensional vectors (lists
of numbers). The key insight is that *words that appear in similar contexts tend to have similar meanings.*


*How Word2Vec Learns (Simplified):*

Word2Vec works by training a shallow neural network on a large corpus of text. It has two main architectures:

1. *Skip-gram:* Predicts surrounding context words given a target word.
2. *CBOW (Continuous Bag of Words):* Predicts a target word given its surrounding context words.

During training, the model adjusts the word vectors so that words frequently appearing together are mapped
closer to each other in the vector space, while words that rarely appear together are mapped further apart.


#### What these Vectors Represent

* *Meaning:* Words with similar meanings (e.g., "king" and "queen") will have vectors that are close to each other.

* *Relationships:* The relationships between words can also be captured. For example, the
  vector difference between "king" and "man" might be similar to the vector difference between
  "queen" and "woman." This allows for famous analogies like "king - man + woman = queen."

* *Context:* The vectors implicitly capture the typical contexts in which words appear.


#### Why are they Important?

* *Semantic Understanding:* They provide a way for machines to grasp the semantic meaning
  of words, going beyond simple string matching.

* *Feature Representation:* They serve as powerful features for various downstream NLP tasks like:
    * Text classification
    * Sentiment analysis
    * Machine translation
    * Information retrieval

* *Reduced Dimensionality:* They compress high-dimensional sparse representations (like one-hot
  encodings) into more manageable, dense vectors, improving computational efficiency and model
  performance.

In essence, word embeddings like Word2Vec transform words from abstract symbols into meaningful
numerical representations, enabling computers to process and understand human language in a more
sophisticated way.


#### Skip-gram model

```mermaid
graph TD
    subgraph Input Layer
        A[One-hot encoded target word] --> B{Embedding Layer};
    end

    subgraph Hidden Layer (Projection Layer)
        B --> C[Word Embedding Vector];
    end

    subgraph Output Layer
        C --> D[Softmax Layer];
        D --> E[Predicted Context Words (Probabilities)];
    end

    A -- "Input Word (e.g., 'cat')" --> B;
    B -- "Learned Embedding Matrix" --> C;
    C -- "Predicts Neighbors" --> D;
    D -- "Compare to Actual Neighbors" --> E;

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#9cf,stroke:#333,stroke-width:2px
    style C fill:#9cf,stroke:#333,stroke-width:2px
    style D fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#f9f,stroke:#333,stroke-width:2px
```
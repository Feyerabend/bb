
## Endorsement

In a moment where public debates about artificial intelligence are often framed in terms
of speculative doom or abstract philosophical critique, a different vision of AI is being
advanced--not one built on hypotheticals or theoretical purity, but on actual capability,
empirical success, and measurable impact. Researchers and entrepreneurs like *Demis Hassabis*,
*Sam Altman*, and their collaborators at institutions like DeepMind, OpenAI, and Anthropic
represent this forward-looking, practically grounded tradition. Rather than being constrained
by analogies to human cognition or debates about consciousness, these figures embrace AI
as a new kind of tool--not a person, not a mind, but a scalable system that can solve problems
once thought intractable.

At the heart of this vision is a focus not on what AI lacks, but on what it can already
do--and has demonstrably accomplished. Demis Hassabis's DeepMind made history not by imitating
human intelligence, but by surpassing it in specific, formally bounded domains. AlphaFold,
for instance, achieved a breakthrough in structural biology that had eluded human researchers
for decades: it predicted the 3D structure of proteins with remarkable accuracy, revolutionising
biochemistry and accelerating drug discovery. This is not a metaphor for intelligence--it
is real scientific advancement, delivered at scale, enabled by machine learning.

Sam Altman, through OpenAI, has overseen the development of systems like GPT-3, GPT-4, and
the broader ChatGPT platform--not as experiments in cognitive realism, but as powerful
general-purpose tools with practical applications in education, programming, healthcare,
and beyond. These models, while not "understanding" in the human sense, nonetheless support
a wide range of tasks with fluency, consistency, and accessibility that make them useful
in ways that rival or exceed earlier AI paradigms.

Where critics like Mitchell and Marcus raise valid concerns about brittleness or compositionality,
proponents of deep learning reply with empirical progress: GPT-4, for example, exhibits
substantial improvements in multi-step reasoning, few-shot learning, and even complex
programming tasks. And with fine-tuning, tool use, and multi-modal extensions, these systems
are increasingly being used as assistants--not autonomous agents, but collaborators that
help people think, write, code, translate, and learn.

This tradition is not uninterested in philosophy--but it treats cognition as something to be
emulated where useful, not reverse-engineered at the cost of pragmatism. The idea, as Altman
has framed it, is not to rebuild the human brain in silicon, but to construct useful intelligence,
aligned with human values and broadly distributed to empower individuals. Tools like Copilot,
ChatGPT, Claude, and Gemini already empower millions of users daily, lowering barriers to entry
in software engineering, academic writing, legal research, and creative work.

Even in healthcare, AI systems are now being co-developed with clinicians to aid in diagnostics,
radiology, and clinical trial design. Google DeepMind's MedPaLM shows promise in answering medical
questions at near-expert level, and clinical LLMs are being tested for triage support, error
checking, and more efficient documentation. These systems may not "understand" illness as a doctor
does, but they extend human reach, reduce cognitive burden, and make expert-level resources
more widely available.

For Hassabis and others, the future of AI is not about imitation, but augmentation. They acknowledge
current limitations, but treat them as engineering challenges--not as fundamental blocks to progress.
What matters is outcomes: drugs discovered faster, code written more securely, languages translated
more equitably, and ideas made more accessible across educational divides.

Where some see brittleness, they see iteration. Where others point to gaps in symbolic reasoning,
they point to emergent behavior in large-scale systems that wasn't explicitly programmed but arises
nonetheless--suggesting that cognition, too, may be partly scalable. OpenAI's own experiments with
tool-use, memory, and self-reflection (e.g., with ReflectionGPT and AutoGPT variants) suggest a
roadmap toward more grounded, structured systems--but built from working pieces, not speculative
hybrids.

These builders and researchers do not deny that caution is needed; indeed, both OpenAI and DeepMind
have robust AI safety teams and internal governance models. But the central ethos is one of optimism,
tempered by engineering discipline. In contrast to those who warn of philosophical inadequacy,
this camp asks: What can AI do today? What problems can it help us solve? And crucially:
How do we distribute these benefits responsibly?

This tradition looks less to Searle or Davidson than to Turing and von Neumann--visionaries who
saw in computation not a threat to understanding, but a way to extend it. It embraces a model of
progress in which usefulness, reliability, and iterative refinement are more important than
perfect mimicry of human minds. And in doing so, it brings AI closer to the sciences it increasingly
aids: empirical, adaptive, and transformative.


### Reference



### Projects

__1. Simulate Drug Discovery with AlphaFold Data__

Use publicly available AlphaFold-predicted protein structures to simulate a simplified drug
screening pipeline, demonstrating how ML-derived biology accelerates previously slow research loops.

__2. Implement LLM-Based Teaching Assistant__

Build a classroom assistant using GPT-4 or Claude that answers student questions, gives hints
on programming assignments, and helps draft essays--then evaluate user satisfaction, learning
outcomes, and instructor load.

__3. Compare AI-Assisted vs. Manual Coding Performance__

Use Copilot or similar tools to assist in solving a suite of LeetCode problems. Compare against
unaided attempts in terms of time, correctness, and complexity.

__4. Build a Domain-Specific LLM Assistant__

Fine-tune a small language model on a specific domain (e.g. environmental law, chemistry papers)
and demonstrate how it outperforms generic LLMs on targeted questions--emphasising practical grounding.

__5. Investigate Emergent Reasoning Patterns__

Test GPT-4 or Claude on step-by-step reasoning problems. Then use prompt engineering or
chain-of-thought techniques to analyse emergent behaviours that approximate compositional logic.

__6. Visualise Impact of AI on Scientific Publication Trends__

Analyse publication databases before and after the release of tools like AlphaFold or Codex
to show shifts in research pace, topic frequency, or co-authorship networks.

__7. Evaluate Multi-modal Learning in Educational Contexts__

Use a vision-language model (like Gemini or GPT-4o) to assist in math or physics problems
involving diagrams. Test whether multi-modal grounding helps with deeper understanding.

